

\section{Contextual Rewriting}

Arbitrary extra contextual rewrites can be introduced by
using "congurence rules".  These are theorems of a particular
shape.

The general form must be:
\begin{verbatim}
|- !x1 x1' ... xn xn'.
     (!v11...v1m. x1 v11 ... v1m = x1' v11 ... v1m) ==>
     (!v21...v2m. [P[x1,v21,...v2m] ==>] x2 v21 ... v2m = x2' v21 ... v2m) ==>
     ...
     F[x1,x2,..,xn] = F[x1',x2',..,xn']
\end{verbatim}
That probably doesn't make much sense.  Think of F as the construct
over which you are expressing the congruence.  Think of x1,x2,...xn
as the sub-constructs which are being rewritten, some of them under
additional assumptions.  The implications (one on each line in the 
sample above) state the necessary results which need to be derived 
about the subcomponents before the congruence can be deduced.  Some
of these subcomponenets may be rewritten with extra assumpions - this
is indicated by P[x1] above.

Some subcomponents may also be functions - in this case we want
to rewrite them as applied to sets of variables v1...v1m etc.
See the rule for restricted quantifiers for examples.
The simplifier does a degree of higher order matching when
these variables are specified.

Some examples:
\begin{verbatim}
 |- !g g' t t' e e'.
       (g = g') ==>
       (g ==> (t = t')) ==>
       (~g ==> (e = e')) ==>
       ((g => t | e) = (g' => t' | e')) : thm

  |- !P P' Q Q'.
       (!x. P x = P' x) ==>
       (!x. P x ==> (Q x = Q' x)) ==>
       (RES_EXISTS P Q = RES_EXISTS P' Q') : thm
\end{verbatim}




\section{Contextual Rewriting}

Contextual rewriting lets you add assumptions to your rewrite
set as you descend into a term.

The most obvious contextual rewrite is for terms of the form:
\begin{verbatim}
		P ==> Q
\end{verbatim}
The simplifier can use any rewrites that come from P
when rewriting Q.  

Other contextual rewrites included are:
\begin{verbatim}
	      P => T1 | T2  (assume P when rewriting T1, ~P when rewriting T2)
	      !x::P. T1[x]  (assume "P x" when rewriting T1[x])
	      ?x::P. T1[x]  (assume "P x" when rewriting T1[x])
	      \x::P. T1[x]  (assume "P x" when rewriting T1[x])
\end{verbatim}


\section{Conditional Rewriting}

Any theorem which can be converted to the form
\begin{verbatim}
	|- P1[x1...xm] ==> ... Pm[x1...xm] ==> (T1[x1...xm] = T2 [x1...xm])
\end{verbatim}
can potentially be used as a conditional rewrite.  This is 
like the existing conditional rewriting in HOL.  However, the process
of producing conditional rewrites is automated by setting the "rewrite
maker" in your simpset.  For example, res\_quan\_ss (the simpset
for the res\_quan library) extends the rewrite maker to be able to convert
theorems such as:
\begin{verbatim}
|- !n. !w ::(PWORDLEN n). NBWORD n (BNVAL w) = w : thm
\end{verbatim}
into conditional rewrites.  The above theorem will become:
\begin{verbatim}
|- PWORDLEN n w ==> (NBWORD n (BNVAL w) = w)
\end{verbatim}
and will only be applied if "PWORDLEN n w" can be solved in the
context-of-application.  Here "n" and "w" will be instantiated
to the correct arguments.

\section{Adding Arbitrary Conversions}

You can add conversions to the simpset which might (potentially)
get applied at every point in the term. 

Simpsets can contain arbitrary user conversions, as well as
rewrites and contextual-rewrites.  Conversions are keyed by
term patterns (implemented using termnets).  Thus a conversion
won't even be called if the target term doesn't match 
(in the termnet sense of matching) it's key.  This just acts
as a simple optimization/filter.

For example, BETA\_CONV is keyed on the term 
\begin{verbatim}
(--`(\x. t) y`--).  
\end{verbatim}
\footnote{I'm not sure if the HOL implementation of term nets handles keys
which contain abstractions efficiently}


\section{AC Rewriting/Rewritng with Permutative Theorems}

Normally, these rewrites such as:
\begin{verbatim}
ADD_SYM |- !x y. x + y = y + x
\end{verbatim}
cause {\tt REWRITE\_TAC} to loop.  However, the simplifier only applies
them in certain situations, namely when the term they produce
is strictly less than the original term according to a built in
term ordering.  \footnote{Note that the term ordering in hol90
is broken, so a much less efficient term ordering is defined in
the simplifier.  This should be fixed in the next release of hol90.}

By putting three theorems in your simpset: associativity, commutativity and
left-commutativity, you get AC-normalization for free.  You
have to be careful about this:
\begin{itemize}
   \item The associative laws must always be oriented from left
   to right, as in {\tt |- f(f(x,y),z)) = f(x,f(y,z))}.  Otherwise
   HOL will loop.
   \item You need to add left-commutativity to get full normalization:
   {\tt |- f(x,f(y,z)) = f(y,f(x,z))}.  This follows easily
   from associativity and commutativity.
\end{itemize}

AC normalization with the simplifier is comparatively expensive.
Terms of  20 or more operations can take a long time to
normalize.  Smaller terms are OK, but in general it may be a problem
to include AC normalization in all your simpsets.  Experiment
and see!

See the Isabelle reference manual chapter 10 for more details on
AC normalization.

\subsection{Examples of AC Normalization}

\begin{verbatim}
- SIMP_CONV ac_arithmetic_ss (--`(x + 3) + 4 + (y + 2)`--);
val it = |- (x + 3) + 4 + y + 2 = 2 + 3 + 4 + x + y : thm
\end{verbatim}


\chapter{Efficiency and Memory Usage}

\section{Memory Usage}

After loading hol\_ss, arith\_ss,reduce\_ss, taut\_ss,
res\_quan\_ss, and pred\_set\_ss:
\begin{verbatim}
[Major collection... 99% used (11674608/11678744), 2300 msec]
\end{verbatim}
Without these:
\begin{verbatim}
[Major collection... 99% used (10103312/10108132), 1950 msec]
\end{verbatim}
